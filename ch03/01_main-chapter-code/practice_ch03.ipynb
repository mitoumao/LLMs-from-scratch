{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### casual attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CasualAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, num_heads, head_dim, dropout, kvq_bias=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        assert (d_out % num_heads == 0), \"d_out must be dividable to num_head.\" \n",
    "        \n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = self.d_out // self.num_heads \n",
    "        self.W_queries = nn.Linear(d_in, d_out, bias=kvq_bias)\n",
    "        self.W_keys = nn.Linear(d_in, d_out, bias=kvq_bias)\n",
    "        self.W_values = nn.Linear(d_in, d_out, bias=kvq_bias)   \n",
    "        self.out_proj = nn.Linear(d_out, d_out) \n",
    "        self.dropout = dropout\n",
    "        self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "        \n",
    "        queries = self.W_queries(x) \n",
    "        keys = self.W_keys(x)\n",
    "        values = self.W_values(x)\n",
    "        \n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        \n",
    "        queries = queries.transpose(1, 2) # b, num_heads, num_token, head_dim\n",
    "        keys = keys.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "        \n",
    "        attention_scores = queries @ keys.transpose(2, 3)\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "        attention_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "        \n",
    "        attention_weights = torch.softmax(attention_scores / keys.shape[-1]**0.5, dim = -1) \n",
    "        \n",
    "        context_vec = (attention_weights @ values).transpose(1, 2)\n",
    "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
    "        context_vec = self.out_proj(context_vec)\n",
    "        \n",
    "        return context_vec\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1437,  0.2184,  0.0615,  0.2260,  0.0170,  0.1026, -0.0006,\n",
       "           0.1692,  0.0425,  0.0391,  0.0152,  0.0437,  0.1384, -0.1728,\n",
       "           0.0266,  0.0096,  0.1692, -0.2495, -0.1524, -0.0415, -0.0241,\n",
       "           0.0205,  0.0444,  0.0144,  0.3617,  0.0575,  0.1927,  0.0241,\n",
       "          -0.2630, -0.0534,  0.0893, -0.2652, -0.0309,  0.1866,  0.1256,\n",
       "          -0.0518, -0.0425, -0.1486,  0.2586,  0.1293,  0.0839, -0.1025,\n",
       "           0.0048, -0.1052,  0.0315,  0.2913,  0.0947, -0.0315,  0.2273,\n",
       "           0.2647, -0.1179, -0.3454, -0.1044,  0.1476, -0.0801,  0.0619,\n",
       "          -0.3243,  0.0524, -0.0838,  0.0319,  0.2107, -0.0425,  0.1470,\n",
       "          -0.2584],\n",
       "         [ 0.1269,  0.2548,  0.1296,  0.1513,  0.0902,  0.2092,  0.0165,\n",
       "           0.1988,  0.1055,  0.1252,  0.0861,  0.0134,  0.1304, -0.1216,\n",
       "           0.0976, -0.0189,  0.2475, -0.0715, -0.2573, -0.0576, -0.2501,\n",
       "           0.2173,  0.0005,  0.0693,  0.3349,  0.1248,  0.1267, -0.0421,\n",
       "          -0.1279,  0.0631,  0.0861, -0.1621, -0.1222,  0.0593,  0.2814,\n",
       "          -0.0098, -0.0880, -0.0431,  0.3228,  0.1245,  0.0154, -0.0253,\n",
       "           0.0969, -0.1247,  0.0444,  0.3443,  0.0946,  0.0110,  0.2777,\n",
       "           0.4046, -0.1820, -0.3681, -0.1642,  0.1545,  0.0106,  0.1007,\n",
       "          -0.3612,  0.1548,  0.0475,  0.0759,  0.2033,  0.0016,  0.2067,\n",
       "          -0.2014],\n",
       "         [ 0.0959,  0.2252,  0.1178,  0.1193,  0.0592,  0.1839, -0.0147,\n",
       "           0.1760,  0.0814,  0.0730,  0.0699,  0.0417,  0.1224, -0.0670,\n",
       "           0.0753, -0.0402,  0.1629, -0.0690, -0.1950, -0.0217, -0.2164,\n",
       "           0.1561,  0.0093,  0.0516,  0.2808,  0.1227,  0.0862, -0.0107,\n",
       "          -0.0833,  0.0458,  0.0803, -0.1456, -0.0847,  0.0683,  0.2060,\n",
       "          -0.0255, -0.0697, -0.0496,  0.2760,  0.1112,  0.0007, -0.0387,\n",
       "           0.0891, -0.0956,  0.0127,  0.2519,  0.0490,  0.0137,  0.2383,\n",
       "           0.3186, -0.1165, -0.3031, -0.1166,  0.1145,  0.0312,  0.1072,\n",
       "          -0.2747,  0.0926,  0.0170,  0.0287,  0.1347, -0.0040,  0.1841,\n",
       "          -0.1581],\n",
       "         [ 0.1184,  0.2410,  0.1178,  0.1340,  0.0838,  0.2057,  0.0057,\n",
       "           0.1820,  0.0925,  0.1022,  0.0777,  0.0206,  0.1332, -0.0921,\n",
       "           0.0876, -0.0330,  0.2155, -0.0637, -0.2362, -0.0390, -0.2399,\n",
       "           0.1898,  0.0062,  0.0648,  0.3159,  0.1219,  0.1057, -0.0334,\n",
       "          -0.0994,  0.0503,  0.0863, -0.1530, -0.1066,  0.0608,  0.2565,\n",
       "          -0.0160, -0.0833, -0.0493,  0.3031,  0.1117,  0.0024, -0.0448,\n",
       "           0.0926, -0.1087,  0.0242,  0.3034,  0.0762,  0.0114,  0.2634,\n",
       "           0.3698, -0.1579, -0.3335, -0.1455,  0.1348,  0.0213,  0.1027,\n",
       "          -0.3221,  0.1216,  0.0456,  0.0599,  0.1764, -0.0056,  0.2040,\n",
       "          -0.1800],\n",
       "         [ 0.0971,  0.2306,  0.1222,  0.1241,  0.0631,  0.1880, -0.0125,\n",
       "           0.1833,  0.0867,  0.0821,  0.0747,  0.0398,  0.1207, -0.0749,\n",
       "           0.0781, -0.0360,  0.1722, -0.0676, -0.2027, -0.0289, -0.2232,\n",
       "           0.1704,  0.0064,  0.0535,  0.2870,  0.1245,  0.0903, -0.0118,\n",
       "          -0.0895,  0.0515,  0.0809, -0.1456, -0.0902,  0.0646,  0.2166,\n",
       "          -0.0238, -0.0726, -0.0463,  0.2838,  0.1167,  0.0055, -0.0292,\n",
       "           0.0915, -0.1015,  0.0194,  0.2661,  0.0534,  0.0125,  0.2436,\n",
       "           0.3334, -0.1265, -0.3150, -0.1217,  0.1198,  0.0284,  0.1084,\n",
       "          -0.2870,  0.1044,  0.0198,  0.0329,  0.1423, -0.0022,  0.1861,\n",
       "          -0.1624],\n",
       "         [ 0.0852,  0.2375,  0.1404,  0.1149,  0.0688,  0.1976, -0.0156,\n",
       "           0.1981,  0.1008,  0.0965,  0.0893,  0.0420,  0.1114, -0.0713,\n",
       "           0.0881, -0.0357,  0.1752, -0.0443, -0.2121, -0.0362, -0.2549,\n",
       "           0.2053, -0.0033,  0.0575,  0.2758,  0.1370,  0.0800, -0.0126,\n",
       "          -0.0762,  0.0760,  0.0787, -0.1283, -0.1023,  0.0441,  0.2329,\n",
       "          -0.0200, -0.0767, -0.0244,  0.2935,  0.1267,  0.0031,  0.0021,\n",
       "           0.1071, -0.1079,  0.0294,  0.2747,  0.0484,  0.0182,  0.2479,\n",
       "           0.3528, -0.1329, -0.3264, -0.1258,  0.1226,  0.0412,  0.1173,\n",
       "          -0.2917,  0.1277,  0.0267,  0.0334,  0.1355,  0.0102,  0.1869,\n",
       "          -0.1522]],\n",
       "\n",
       "        [[ 0.1437,  0.2184,  0.0615,  0.2260,  0.0170,  0.1026, -0.0006,\n",
       "           0.1692,  0.0425,  0.0391,  0.0152,  0.0437,  0.1384, -0.1728,\n",
       "           0.0266,  0.0096,  0.1692, -0.2495, -0.1524, -0.0415, -0.0241,\n",
       "           0.0205,  0.0444,  0.0144,  0.3617,  0.0575,  0.1927,  0.0241,\n",
       "          -0.2630, -0.0534,  0.0893, -0.2652, -0.0309,  0.1866,  0.1256,\n",
       "          -0.0518, -0.0425, -0.1486,  0.2586,  0.1293,  0.0839, -0.1025,\n",
       "           0.0048, -0.1052,  0.0315,  0.2913,  0.0947, -0.0315,  0.2273,\n",
       "           0.2647, -0.1179, -0.3454, -0.1044,  0.1476, -0.0801,  0.0619,\n",
       "          -0.3243,  0.0524, -0.0838,  0.0319,  0.2107, -0.0425,  0.1470,\n",
       "          -0.2584],\n",
       "         [ 0.1269,  0.2548,  0.1296,  0.1513,  0.0902,  0.2092,  0.0165,\n",
       "           0.1988,  0.1055,  0.1252,  0.0861,  0.0134,  0.1304, -0.1216,\n",
       "           0.0976, -0.0189,  0.2475, -0.0715, -0.2573, -0.0576, -0.2501,\n",
       "           0.2173,  0.0005,  0.0693,  0.3349,  0.1248,  0.1267, -0.0421,\n",
       "          -0.1279,  0.0631,  0.0861, -0.1621, -0.1222,  0.0593,  0.2814,\n",
       "          -0.0098, -0.0880, -0.0431,  0.3228,  0.1245,  0.0154, -0.0253,\n",
       "           0.0969, -0.1247,  0.0444,  0.3443,  0.0946,  0.0110,  0.2777,\n",
       "           0.4046, -0.1820, -0.3681, -0.1642,  0.1545,  0.0106,  0.1007,\n",
       "          -0.3612,  0.1548,  0.0475,  0.0759,  0.2033,  0.0016,  0.2067,\n",
       "          -0.2014],\n",
       "         [ 0.0959,  0.2252,  0.1178,  0.1193,  0.0592,  0.1839, -0.0147,\n",
       "           0.1760,  0.0814,  0.0730,  0.0699,  0.0417,  0.1224, -0.0670,\n",
       "           0.0753, -0.0402,  0.1629, -0.0690, -0.1950, -0.0217, -0.2164,\n",
       "           0.1561,  0.0093,  0.0516,  0.2808,  0.1227,  0.0862, -0.0107,\n",
       "          -0.0833,  0.0458,  0.0803, -0.1456, -0.0847,  0.0683,  0.2060,\n",
       "          -0.0255, -0.0697, -0.0496,  0.2760,  0.1112,  0.0007, -0.0387,\n",
       "           0.0891, -0.0956,  0.0127,  0.2519,  0.0490,  0.0137,  0.2383,\n",
       "           0.3186, -0.1165, -0.3031, -0.1166,  0.1145,  0.0312,  0.1072,\n",
       "          -0.2747,  0.0926,  0.0170,  0.0287,  0.1347, -0.0040,  0.1841,\n",
       "          -0.1581],\n",
       "         [ 0.1184,  0.2410,  0.1178,  0.1340,  0.0838,  0.2057,  0.0057,\n",
       "           0.1820,  0.0925,  0.1022,  0.0777,  0.0206,  0.1332, -0.0921,\n",
       "           0.0876, -0.0330,  0.2155, -0.0637, -0.2362, -0.0390, -0.2399,\n",
       "           0.1898,  0.0062,  0.0648,  0.3159,  0.1219,  0.1057, -0.0334,\n",
       "          -0.0994,  0.0503,  0.0863, -0.1530, -0.1066,  0.0608,  0.2565,\n",
       "          -0.0160, -0.0833, -0.0493,  0.3031,  0.1117,  0.0024, -0.0448,\n",
       "           0.0926, -0.1087,  0.0242,  0.3034,  0.0762,  0.0114,  0.2634,\n",
       "           0.3698, -0.1579, -0.3335, -0.1455,  0.1348,  0.0213,  0.1027,\n",
       "          -0.3221,  0.1216,  0.0456,  0.0599,  0.1764, -0.0056,  0.2040,\n",
       "          -0.1800],\n",
       "         [ 0.0971,  0.2306,  0.1222,  0.1241,  0.0631,  0.1880, -0.0125,\n",
       "           0.1833,  0.0867,  0.0821,  0.0747,  0.0398,  0.1207, -0.0749,\n",
       "           0.0781, -0.0360,  0.1722, -0.0676, -0.2027, -0.0289, -0.2232,\n",
       "           0.1704,  0.0064,  0.0535,  0.2870,  0.1245,  0.0903, -0.0118,\n",
       "          -0.0895,  0.0515,  0.0809, -0.1456, -0.0902,  0.0646,  0.2166,\n",
       "          -0.0238, -0.0726, -0.0463,  0.2838,  0.1167,  0.0055, -0.0292,\n",
       "           0.0915, -0.1015,  0.0194,  0.2661,  0.0534,  0.0125,  0.2436,\n",
       "           0.3334, -0.1265, -0.3150, -0.1217,  0.1198,  0.0284,  0.1084,\n",
       "          -0.2870,  0.1044,  0.0198,  0.0329,  0.1423, -0.0022,  0.1861,\n",
       "          -0.1624],\n",
       "         [ 0.0852,  0.2375,  0.1404,  0.1149,  0.0688,  0.1976, -0.0156,\n",
       "           0.1981,  0.1008,  0.0965,  0.0893,  0.0420,  0.1114, -0.0713,\n",
       "           0.0881, -0.0357,  0.1752, -0.0443, -0.2121, -0.0362, -0.2549,\n",
       "           0.2053, -0.0033,  0.0575,  0.2758,  0.1370,  0.0800, -0.0126,\n",
       "          -0.0762,  0.0760,  0.0787, -0.1283, -0.1023,  0.0441,  0.2329,\n",
       "          -0.0200, -0.0767, -0.0244,  0.2935,  0.1267,  0.0031,  0.0021,\n",
       "           0.1071, -0.1079,  0.0294,  0.2747,  0.0484,  0.0182,  0.2479,\n",
       "           0.3528, -0.1329, -0.3264, -0.1258,  0.1226,  0.0412,  0.1173,\n",
       "          -0.2917,  0.1277,  0.0267,  0.0334,  0.1355,  0.0102,  0.1869,\n",
       "          -0.1522]]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "inputs = torch.rand(6, 3)\n",
    "inputs = torch.stack((inputs, inputs), dim = 0)\n",
    "d_in = inputs.shape[2]\n",
    "d_out = 64 \n",
    "context_length = 6\n",
    "num_heads = 4\n",
    "head_dim = 16\n",
    "dropout = 0.0\n",
    "ca = CasualAttention(d_in, d_out, context_length, num_heads, head_dim, dropout)\n",
    "ca(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_scratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
